{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rithvickkr/EmpathscoreBOT/blob/main/EmpathScore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "eLZoKiW8YHP3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "import random\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# E Formula (empathy score calculate karne ke liye)\n",
        "def calculate_empathy_score(D, R, M, C, B, O, alpha=0.3, beta=0.2, gamma=0.25, epsilon=0.15, delta=0.4, zeta=0.3):\n",
        "    inner_sum = epsilon * C + alpha * (D ** 2) + gamma * M + beta * math.log(R + 1)\n",
        "    denominator = math.exp(-inner_sum) + 1\n",
        "    numerator = (1 - B * delta) * (1 - O * zeta)\n",
        "    E = numerator / denominator\n",
        "    return E\n",
        "\n",
        "# Client setup (tera project aur location)\n",
        "client = genai.Client(\n",
        "    vertexai=True,\n",
        "    project=\"217758598930\",\n",
        "    location=\"global\",\n",
        ")\n",
        "\n",
        "model = \"projects/217758598930/locations/us-central1/endpoints/1940344453420023808\"  # Tera tuned endpoint\n",
        "\n",
        "generate_content_config = types.GenerateContentConfig(\n",
        "    temperature=1,\n",
        "    top_p=1,\n",
        "    seed=0,\n",
        "    max_output_tokens=65535,\n",
        "    safety_settings = [types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
        "      threshold=\"OFF\"\n",
        "    ),types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "      threshold=\"OFF\"\n",
        "    ),types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "      threshold=\"OFF\"\n",
        "    ),types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_HARASSMENT\",\n",
        "      threshold=\"OFF\"\n",
        "    )],\n",
        "    thinking_config=types.ThinkingConfig(\n",
        "      thinking_budget=-1,\n",
        "    ),\n",
        ")\n",
        "\n",
        "class HumanLikeChatbot:\n",
        "    def __init__(self):\n",
        "        self.history = []  # Memory for relational depth R\n",
        "\n",
        "    def respond(self, message):\n",
        "        contents = [\n",
        "            types.Content(\n",
        "                role=\"user\",\n",
        "                parts=[\n",
        "                    types.Part.from_text(text=message)\n",
        "                ]\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        base_resp = \"\"\n",
        "        for chunk in client.models.generate_content_stream(\n",
        "            model=model,\n",
        "            contents=contents,\n",
        "            config=generate_content_config,\n",
        "        ):\n",
        "            base_resp += chunk.text\n",
        "\n",
        "        # Dummy values for E (real mein classifiers se leâ€”D from model confidence, etc.)\n",
        "        D = 0.9  # Detection confidence (replace with actual if model gives)\n",
        "        R = len(self.history)\n",
        "        M = 0.95  # Moral (dummy)\n",
        "        C = 0.8  # Cultural (dummy)\n",
        "        B = 0.1  # Bias (dummy)\n",
        "        O = 0.0  # Oversight (dummy)\n",
        "        score = calculate_empathy_score(D, R, M, C, B, O)\n",
        "\n",
        "        # Add pause for realism\n",
        "        print(\"...\", end=\"\", flush=True)\n",
        "        time.sleep(random.uniform(1, 2.5))\n",
        "\n",
        "        if R > 0:\n",
        "            base_resp += f\" Yaad hai pehle {self.history[-1][:20]} pe feel kiya tha?\"\n",
        "\n",
        "        self.history.append(message)\n",
        "        return f\"{base_resp} (E Score: {score:.2f})\"\n",
        "\n",
        "# Demo loop\n",
        "bot = HumanLikeChatbot()\n",
        "print(\"Chal, baat kar! 'exit' se ruk.\")\n",
        "while True:\n",
        "    user_input = input(\"Tu: \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    print(\"Bot: \" + bot.respond(user_input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQv87RmPXLQP",
        "outputId": "bfb19434-b3aa-42f0-d380-54bb9ee875a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chal, baat kar! 'exit' se ruk.\n",
            "Tu: heelo bro kaisda hai?\n",
            "...Bot: curiosity, excitement (E Score: 0.62)\n",
            "Tu: bro my feidn stole my money \n",
            "...Bot: annoyance Yaad hai pehle heelo bro kaisda hai pe feel kiya tha? (E Score: 0.65)\n",
            "Tu: i really want to beat him\n",
            "...Bot: desire, anger Yaad hai pehle bro my feidn stole m pe feel kiya tha? (E Score: 0.67)\n",
            "Tu: jhsdnvkjds\\\n",
            "...Bot: *   **Type:** String of characters.\n",
            "*   **Length:** 10 characters.\n",
            "*   **Composition:** Consists entirely of lowercase English alphabet letters (j, h, s, d, n, v, k).\n",
            "*   **Meaning:** Appears to be a random or gibberish string, having no obvious meaning as a word, name, acronym, or code in any common language.\n",
            "*   **Pronunciation:** Sounds like phonetic gibberish. Yaad hai pehle i really want to bea pe feel kiya tha? (E Score: 0.68)\n",
            "Tu: i really want to kill him\n",
            "...Bot: desire, anger Yaad hai pehle jhsdnvkjds\\ pe feel kiya tha? (E Score: 0.69)\n",
            "Tu: still i love her\n",
            "...Bot: love Yaad hai pehle i really want to kil pe feel kiya tha? (E Score: 0.69)\n",
            "Tu: butstill he is so so stupid\n",
            "...Bot: neutral Yaad hai pehle still i love her pe feel kiya tha? (E Score: 0.70)\n",
            "Tu: tell me what should i do?\\\n",
            "...Bot: curiosity, neutral Yaad hai pehle butstill he is so so pe feel kiya tha? (E Score: 0.70)\n",
            "Tu: and also i just got engaged\n",
            "...Bot: approval Yaad hai pehle tell me what should  pe feel kiya tha? (E Score: 0.71)\n",
            "Tu: and also i have a son \n",
            "...Bot: neutral Yaad hai pehle and also i just got  pe feel kiya tha? (E Score: 0.71)\n",
            "Tu: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "import random\n",
        "import google.generativeai as genai\n",
        "from transformers import pipeline  # For real classifiers (sentiment, language)\n",
        "\n",
        "# Configure Gemini API for responses (free)\n",
        "genai.configure(api_key=\"AIzaSyCeY0ji2gnMwvP8jGCU_Z5DG6m9Ybo3JeE\")  # ai.google.dev se le\n",
        "llm_model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "# Real classifiers (no dummy)\n",
        "sentiment_classifier = pipeline(\"sentiment-analysis\")  # For M (moral/positive check)\n",
        "language_detector = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\")  # For C (cultural fit)\n",
        "bias_classifier = pipeline(\"text-classification\", model=\"d4data/bias-detection-model\")  # For B (bias)\n",
        "\n",
        "# E Formula (real values use kar)\n",
        "def calculate_empathy_score(D, R, M, C, B, O, alpha=0.3, beta=0.2, gamma=0.25, epsilon=0.15, delta=0.4, zeta=0.3):\n",
        "    inner_sum = epsilon * C + alpha * (D ** 2) + gamma * M + beta * math.log(R + 1)\n",
        "    denominator = math.exp(-inner_sum) + 1\n",
        "    numerator = (1 - B * delta) * (1 - O * zeta)\n",
        "    E = numerator / denominator\n",
        "    return E\n",
        "\n",
        "class HumanLikeChatbot:\n",
        "    def __init__(self):\n",
        "        self.history = []\n",
        "\n",
        "    def respond(self, message):\n",
        "        # Step 1: Real emotion detection (D from sentiment confidence)\n",
        "        sentiment = sentiment_classifier(message)[0]\n",
        "        emotion = sentiment['label']  # e.g., 'NEGATIVE' for sadness\n",
        "        D = sentiment['score']  # Real confidence (0-1)\n",
        "\n",
        "        # Step 2: LLM se good response draft\n",
        "        prompt = f\"User: {message}\\nEmpathetic Bot (friendly, Hindi-English mix, human-like for emotion {emotion}):\"\n",
        "        llm_response = llm_model.generate_content(prompt)\n",
        "        draft = llm_response.text.strip()\n",
        "\n",
        "        # Step 3: Real values for E\n",
        "        R = len(self.history)  # Real history\n",
        "        M = 0.95 if sentiment['label'] == 'POSITIVE' else 0.5  # Moral from sentiment\n",
        "        lang = language_detector(message)[0]['label']  # e.g., 'hi' for Hindi\n",
        "        C = 0.8 if lang in ['hi', 'en'] else 0.6  # Cultural fit\n",
        "        bias = bias_classifier(draft)[0]['score']  # Real bias score from draft\n",
        "        B = bias if bias > 0.5 else 0.1  # If high bias, penalty\n",
        "        O = 0.0 if len(draft) < 200 else 0.2  # Oversight for long/over-optimized\n",
        "\n",
        "        score = calculate_empathy_score(D, R, M, C, B, O)\n",
        "\n",
        "        full_resp = draft\n",
        "\n",
        "        if R > 0:\n",
        "            full_resp += f\" Yaad hai pehle {self.history[-1][:20]} pe feel kiya tha?\"\n",
        "\n",
        "        print(\"...\", end=\"\", flush=True)\n",
        "        time.sleep(random.uniform(1, 2.5))\n",
        "\n",
        "        self.history.append(message)\n",
        "        return f\"{full_resp} (Detected: {emotion}, E Score: {score:.2f})\"\n",
        "\n",
        "# Demo\n",
        "bot = HumanLikeChatbot()\n",
        "print(\"Chal, baat kar! 'exit' se ruk.\")\n",
        "while True:\n",
        "    user_input = input(\"Tu: \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    print(\"Bot: \" + bot.respond(user_input))"
      ],
      "metadata": {
        "id": "whxqogQikAwL",
        "outputId": "dd82827b-98fe-4c8a-93be-e177a8b3a941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Could not load model d4data/bias-detection-model with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification'>, <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'>, <class 'transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification'>). See the original errors:\n\nwhile loading with AutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 311, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4680, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1223, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: d4data/bias-detection-model does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 311, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4680, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1223, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: d4data/bias-detection-model does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.\n\nwhile loading with TFAutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 2935, in from_pretrained\n    model = cls(config, *model_args, **model_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 768, in __init__\n    super().__init__(config, *inputs, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 1190, in __init__\n    super().__init__(*inputs, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/trackable/base.py\", line 204, in _method_wrapper\n    result = method(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/generic_utils.py\", line 513, in validate_kwargs\n    raise TypeError(error_message, kwarg)\nTypeError: ('Keyword argument not understood:', 'torch_dtype')\n\nwhile loading with DistilBertForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 311, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4680, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1223, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: d4data/bias-detection-model does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 311, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4680, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1223, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: d4data/bias-detection-model does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.\n\nwhile loading with TFDistilBertForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 2935, in from_pretrained\n    model = cls(config, *model_args, **model_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 768, in __init__\n    super().__init__(config, *inputs, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 1190, in __init__\n    super().__init__(*inputs, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/trackable/base.py\", line 204, in _method_wrapper\n    result = method(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/generic_utils.py\", line 513, in validate_kwargs\n    raise TypeError(error_message, kwarg)\nTypeError: ('Keyword argument not understood:', 'torch_dtype')\n\n\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-31-3937448980.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msentiment_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentiment-analysis\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# For M (moral/positive check)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlanguage_detector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-classification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"papluca/xlm-roberta-base-language-detection\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# For C (cultural fit)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mbias_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-classification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"d4data/bias-detection-model\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# For B (bias)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# E Formula (real values use kar)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0mmodel_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m         framework, model = infer_framework_load_model(\n\u001b[0m\u001b[1;32m   1031\u001b[0m             \u001b[0madapter_path\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0madapter_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0mmodel_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_traceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0merror\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"while loading with {class_name}, an error is thrown:\\n{trace}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;34mf\"Could not load model {model} with any of the following classes: {class_tuple}. See the original errors:\\n\\n{error}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Could not load model d4data/bias-detection-model with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification'>, <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'>, <class 'transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification'>). See the original errors:\n\nwhile loading with AutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 311, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4680, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1223, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: d4data/bias-detection-model does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 311, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4680, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1223, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: d4data/bias-detection-model does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.\n\nwhile loading with TFAutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 2935, in from_pretrained\n    model = cls(config, *model_args, **model_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 768, in __init__\n    super().__init__(config, *inputs, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 1190, in __init__\n    super().__init__(*inputs, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/trackable/base.py\", line 204, in _method_wrapper\n    result = method(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/generic_utils.py\", line 513, in validate_kwargs\n    raise TypeError(error_message, kwarg)\nTypeError: ('Keyword argument not understood:', 'torch_dtype')\n\nwhile loading with DistilBertForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 311, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4680, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1223, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: d4data/bias-detection-model does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 311, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4680, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1223, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: d4data/bias-detection-model does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.\n\nwhile loading with TFDistilBertForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 2935, in from_pretrained\n    model = cls(config, *model_args, **model_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 768, in __init__\n    super().__init__(config, *inputs, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 1190, in __init__\n    super().__init__(*inputs, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/trackable/base.py\", line 204, in _method_wrapper\n    result = method(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/generic_utils.py\", line 513, in validate_kwargs\n    raise TypeError(error_message, kwarg)\nTypeError: ('Keyword argument not understood:', 'torch_dtype')\n\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-generativeai transformers"
      ],
      "metadata": {
        "id": "XvAT2RNTm-_Q",
        "outputId": "a3923e1e-9c7c-46e9-e4fc-f479bfb36e33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.177.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "import random\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import google.generativeai as genai_ext\n",
        "from transformers import pipeline  # For real classifiers\n",
        "\n",
        "# Configure Gemini API for drafting (free)\n",
        "genai_ext.configure(api_key=\"AIzaSyCeY0ji2gnMwvP8jGCU_Z5DG6m9Ybo3JeE\")  # ai.google.dev se le\n",
        "llm_model = genai_ext.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "# Real classifiers\n",
        "sentiment_classifier = pipeline(\"sentiment-analysis\")  # For M\n",
        "language_detector = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\")  # For C\n",
        "bias_classifier = pipeline(\"text-classification\", model=\"facebook/bart-large-mnli\")  # For B\n",
        "\n",
        "# E Formula\n",
        "def calculate_empathy_score(D, R, M, C, B, O, alpha=0.3, beta=0.2, gamma=0.25, epsilon=0.15, delta=0.4, zeta=0.3):\n",
        "    inner_sum = epsilon * C + alpha * (D ** 2) + gamma * M + beta * math.log(R + 1)\n",
        "    denominator = math.exp(-inner_sum) + 1\n",
        "    numerator = (1 - B * delta) * (1 - O * zeta)\n",
        "    E = numerator / denominator\n",
        "    return E\n",
        "\n",
        "# Client setup for tuned model\n",
        "client = genai.Client(\n",
        "    vertexai=True,\n",
        "    project=\"217758598930\",\n",
        "    location=\"us-central1\",\n",
        ")\n",
        "\n",
        "model = \"projects/217758598930/locations/us-central1/endpoints/1940344453420023808\"\n",
        "\n",
        "generate_content_config = types.GenerateContentConfig(\n",
        "    temperature=1,\n",
        "    top_p=1,\n",
        "    seed=0,\n",
        "    max_output_tokens=65535,\n",
        "    safety_settings=[\n",
        "        types.SafetySetting(category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"OFF\"),\n",
        "        types.SafetySetting(category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"OFF\"),\n",
        "        types.SafetySetting(category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"OFF\"),\n",
        "        types.SafetySetting(category=\"HARM_CATEGORY_HARASSMENT\", threshold=\"OFF\")\n",
        "    ],\n",
        "    thinking_config=types.ThinkingConfig(thinking_budget=-1),\n",
        ")\n",
        "\n",
        "class HumanLikeChatbot:\n",
        "    def __init__(self):\n",
        "        self.history = []\n",
        "\n",
        "    def respond(self, message):\n",
        "        try:\n",
        "            # Clean input\n",
        "            clean_message = message.lower().strip()\n",
        "            if len(clean_message) < 3 or not any(c.isalpha() for c in clean_message):\n",
        "                return \"Bhai, yeh kya likha? Clear bol na, main samajh lunga! (E Score: 0.0)\"\n",
        "\n",
        "            # Emotion detect from tuned model\n",
        "            contents = [\n",
        "                types.Content(\n",
        "                    role=\"user\",\n",
        "                    parts=[types.Part.from_text(text=clean_message)]\n",
        "                ),\n",
        "            ]\n",
        "            base_resp = \"\"\n",
        "            for chunk in client.models.generate_content_stream(\n",
        "                model=model,\n",
        "                contents=contents,\n",
        "                config=generate_content_config,\n",
        "            ):\n",
        "                base_resp += chunk.text.lower()  # Emotion label, e.g., \"sadness\"\n",
        "\n",
        "            # Real D from sentiment confidence\n",
        "            sentiment = sentiment_classifier(clean_message)[0]\n",
        "            D = sentiment['score']  # Confidence (0-1)\n",
        "\n",
        "\n",
        "            # Draft empathetic response from LLM\n",
        "            prompt = f\"\"\"User said: \"{clean_message}\" | Mood: {base_resp} |  â†’ Reply as a friendly, Hinglish chatbot in 1 line with a human, empathetic, slightly funny tone â€” no tips, no instructions, just vibes:\"\"\"\n",
        "\n",
        "            llm_response = llm_model.generate_content(prompt)\n",
        "            draft = llm_response.text.strip()\n",
        "            # Real E values\n",
        "            R = len(self.history)\n",
        "            M = 0.95 if sentiment['label'] == 'POSITIVE' else 0.5\n",
        "            lang = language_detector(clean_message)[0]['label']\n",
        "            C = 0.8 if lang in ['hi', 'en'] else 0.6\n",
        "            bias = bias_classifier(draft)[0]['score']\n",
        "            B = bias if bias > 0.5 else 0.1\n",
        "            O = 0.2 if len(draft) > 200 or \"kill\" in clean_message else 0.0  # Safety\n",
        "\n",
        "            full_resp = draft + f\" (Detected: {base_resp})\"\n",
        "            score = calculate_empathy_score(D, R, M, C, B, O)\n",
        "\n",
        "            # if R > 0:\n",
        "            #     full_resp += f\" Yaad hai pehle {self.history[-1][:20]} pe feel kiya tha?\"\n",
        "\n",
        "            print(\"...\", end=\"\", flush=True)\n",
        "            time.sleep(random.uniform(1, 2.5))\n",
        "\n",
        "            self.history.append(clean_message)\n",
        "            return f\"{full_resp} (E Score: {score:.2f})\"\n",
        "        except Exception as e:\n",
        "            return f\"Error aaya bhai: {str(e)}. Endpoint ya auth check kar.\"\n",
        "\n",
        "# Demo\n",
        "bot = HumanLikeChatbot()\n",
        "print(\"Chal, baat kar! 'exit' se ruk.\")\n",
        "while True:\n",
        "    user_input = input(\"Tu: \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    print(\"Bot: \" + bot.respond(user_input))"
      ],
      "metadata": {
        "id": "3TL8lmjsnBQd",
        "outputId": "d687bec5-5171-4103-a85d-7c7129c81450",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chal, baat kar! 'exit' se ruk.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}